{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(dask-read-write)=\n",
    "# 读写数据\n",
    "\n",
    "Dask DataFrame 支持 pandas 中几乎所有的数据读写操作，包括从本地、NFS、HDFS 或 S3 上读写文本文件、Parquet、HDF、JSON 等格式的文件。 {numref}`dask-read-write-operations` 是几个常见的读写操作。\n",
    "\n",
    "```{table} 几个 Dask DataFrame 读写操作示例\n",
    ":name: dask-read-write-operations\n",
    "|   \t| CSV          \t| Parquet          \t| HDF          \t|\n",
    "|---\t|--------------\t|------------------\t|--------------\t|\n",
    "| 读  \t| [`read_csv()`](https://docs.dask.org/en/stable/generated/dask.dataframe.read_csv.html) \t| [`read_parquet()`](https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html) \t| [`read_hdf()`](https://docs.dask.org/en/stable/generated/dask.dataframe.read_hdf.html) \t|\n",
    "| 写  \t| [`to_csv()`](https://docs.dask.org/en/stable/generated/dask.dataframe.to_csv.html)   \t| [`to_parquet()`](https://docs.dask.org/en/stable/generated/dask.dataframe.to_parquet.html)   \t| [`to_hdf()`](https://docs.dask.org/en/stable/generated/dask.dataframe.to_hdf.html)   \t|\n",
    "```\n",
    "\n",
    "## 文件系统\n",
    "\n",
    ":::{tip}\n",
    "当我们在使用 Dask 集群读写数据时，数据源应该存储在 NFS、HDFS、S3 这种共享文件系统，这样所有 Dask Worker 都能访问该数据。\n",
    ":::\n",
    "\n",
    "业内经常使用文件系统前缀（Scheme）来标识不同的文件系统，并调用不同的库来读写文件系统。{numref}`uri-schemes` 是几个 Scheme 例子。一个数据集地址，应该是一个统一资源标识（Uniform Resource Identifier，URI），URI 包括 Scheme 和具体的地址，URI 模板为：`scheme://path/to/data`。比如：`file:///tmp/tripdata.parquet` 或 `s3://tmp/tripdata.parquet`。\n",
    "\n",
    "```{table} URI Scheme\n",
    ":name: uri-schemes\n",
    "|        \t|   本地   \t|   S3  \t|  HDFS \t|\n",
    "|:------:\t|:--------:\t|:-----:\t|:--------:\t|\n",
    "| Scheme \t| file:// \t| s3:// \t| hdfs:// \t|\n",
    "```\n",
    "\n",
    "如果数据集地址没有加任何 Scheme，会假设是本地可访问的文件系统，即该计算节点可以直接读写和访问的文件系统。比如，网络文件系统（Network File System，NFS）是分布式文件系统，假如被挂载到多个计算节点的 `/mnt/nfs` 目录，使用 Dask 读写该目录时，可直接使用 `/mnt/nfs` 这个目录。\n",
    "\n",
    "本书后续介绍的 Ray Data 等也依照同样 URI 和 Scheme 标准。\n",
    "\n",
    "HDFS 和 S3 这样共享文件系统，在企业或组织中被多人共享，因此经常有用户验证的环节，以做好用户之间的数据隔离，避免用户互相修改或删除数据。不同的文件系统有自己的用户验证的方式，比如，S3 用户需要提供令牌（Token），Token 可以通过 `read_*()` 和 `to_*()` (包括 `read_csv()`、`read_parquet`、`to_parquet()` 等方法) 的 `storage_options` 参数传递进来。如果你对用户验证不熟悉，应咨询组织中负责运维管理的同事。\n",
    "\n",
    "## 数据切分与并行读取\n",
    "\n",
    "我们以读取逗号分隔的数值（Comma-Separated Values，CSV）文件为例，来展示 Dask DataFrame 与 pandas 的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/luweizheng/Projects/godaai/distributed-python/ch-dask-dataframe/../data/nyc-flights/*.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import urllib\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "folder_path = os.path.join(os.getcwd(), \"../data/\")\n",
    "download_url = \"https://dp.godaai.org/nyc-flights.zip\"\n",
    "zip_file_path = os.path.join(folder_path, \"nyc-flights.zip\")\n",
    "if not os.path.exists(os.path.join(folder_path, \"nyc-flights\")):\n",
    "    with urllib.request.urlopen(download_url) as response, open(zip_file_path, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "        zf = ZipFile(zip_file_path, 'r')\n",
    "        zf.extractall(folder_path)\n",
    "        zf.close()\n",
    "\n",
    "file_path = os.path.join(folder_path, \"nyc-flights\", \"*.csv\")\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas 和 Dask DataFrame 都提供了 `read_csv()` 方法，用来读取 CSV 文件。Dask 的 [`read_csv()`](https://docs.dask.org/en/stable/generated/dask.dataframe.read_csv.html) 参数与 pandas 几乎一致，可以参考 pandas 的 [`read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)。比如在这个例子中，原始数据有很多列，其中前三列分别为，年：`Year`，月：`Month`，日：`DayofMonth`，`read_csv()` 方法的 `parse_dates` 参数将这三列解析为时间 `datetime64` 类型，并生成一个新的列 `Date`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(file_path, parse_dates={'Date': [0, 1, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 `file_path` 是 `*.csv` 的形式，匹配所有以 `csv` 结尾的文件；而 pandas 的 `read_csv()` 只能读取单个文件，并不支持 `*.csv` 这样的通配符。如果想用 pandas 读文件夹下面的所有以 `csv` 结尾的文件，应该："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(file_path)\n",
    "data = []\n",
    "\n",
    "for p in file_list:\n",
    "    df = pd.read_csv(p, parse_dates={'Date': [0, 1, 2]})\n",
    "    data.append(df)\n",
    "\n",
    "pdf = pd.concat(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看这份数据的前 3 行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991-01-08</td>\n",
       "      <td>2</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991-01-09</td>\n",
       "      <td>3</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991-01-10</td>\n",
       "      <td>4</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "0 1991-01-08          2   1215.0        1215   1340.0        1336   \n",
       "1 1991-01-09          3   1215.0        1215   1353.0        1336   \n",
       "2 1991-01-10          4   1216.0        1215   1332.0        1336   \n",
       "\n",
       "  UniqueCarrier  FlightNum  TailNum  ActualElapsedTime  ...  AirTime  \\\n",
       "0            US        121      NaN               85.0  ...      NaN   \n",
       "1            US        121      NaN               98.0  ...      NaN   \n",
       "2            US        121      NaN               76.0  ...      NaN   \n",
       "\n",
       "   ArrDelay  DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  \\\n",
       "0       4.0       0.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "1      17.0       0.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "2      -4.0       1.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "\n",
       "   Diverted  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991-01-08</td>\n",
       "      <td>2</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991-01-09</td>\n",
       "      <td>3</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991-01-10</td>\n",
       "      <td>4</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "0 1991-01-08          2   1215.0        1215   1340.0        1336   \n",
       "1 1991-01-09          3   1215.0        1215   1353.0        1336   \n",
       "2 1991-01-10          4   1216.0        1215   1332.0        1336   \n",
       "\n",
       "  UniqueCarrier  FlightNum TailNum  ActualElapsedTime  ...  AirTime  ArrDelay  \\\n",
       "0            US        121     NaN               85.0  ...      NaN       4.0   \n",
       "1            US        121     NaN               98.0  ...      NaN      17.0   \n",
       "2            US        121     NaN               76.0  ...      NaN      -4.0   \n",
       "\n",
       "   DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  Diverted  \n",
       "0       0.0     EWR  PIT    319.0     NaN      NaN          0         0  \n",
       "1       0.0     EWR  PIT    319.0     NaN      NaN          0         0  \n",
       "2       1.0     EWR  PIT    319.0     NaN      NaN          0         0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里体现出 Dask DataFrame 与 pandas 的区别：Dask DataFrame 接受 `*.csv` 这样的通配符，可以批量读取文件夹下所有的以 `csv` 结尾的文件。Dask DataFrame 在实现时，先对 `*.csv` 目录进行了遍历，了解到目录下一共有多少个 CSV 文件，并在构建 Task Graph 时，根据文件数量，并行地启动多个 pandas 进程。\n",
    "\n",
    "`ddf.visualize()` 将 Task Graph 进行了可视化，可以看到：目录下有 m 个 CSV 文件，在 Task Graph 中生成 m 个 `read_csv()` 子图，执行时并行地启动 m 个 pandas 的 `read_csv()`；或者说，每个 CSV 文件对应一个 Partition。根据文件数量构建 Task Graph 中的并行粒度是一种最简单的图生成方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1058pt\" height=\"436pt\" viewBox=\"0.00 0.00 1058.21 436.28\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 432.28)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-432.28 1054.21,-432.28 1054.21,4 -4,4\"/>\n",
       "<!-- &#45;4992946418407614279 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>-4992946418407614279</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"80.11\" cy=\"-44.03\" rx=\"44.03\" ry=\"44.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.11\" y=\"-38.23\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- &#45;2791454735931807732 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>-2791454735931807732</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"107.11,-160.07 53.11,-160.07 53.11,-124.07 107.11,-124.07 107.11,-160.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.11\" y=\"-136.27\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- &#45;4992946418407614279&#45;&gt;&#45;2791454735931807732 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>-4992946418407614279-&gt;-2791454735931807732</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.11,-88.55C80.11,-96.66 80.11,-104.95 80.11,-112.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"76.61,-112.36 80.11,-122.36 83.61,-112.36 76.61,-112.36\"/>\n",
       "</g>\n",
       "<!-- &#45;1856934773411932769 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>-1856934773411932769</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"80.11\" cy=\"-276.17\" rx=\"80.11\" ry=\"80.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.11\" y=\"-270.37\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">to_pyarrow_string</text>\n",
       "</g>\n",
       "<!-- &#45;2791454735931807732&#45;&gt;&#45;1856934773411932769 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>-2791454735931807732-&gt;-1856934773411932769</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.11,-160.34C80.11,-167.1 80.11,-175.4 80.11,-184.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"76.61,-184.22 80.11,-194.22 83.61,-184.22 76.61,-184.22\"/>\n",
       "</g>\n",
       "<!-- &#45;66498125748364619 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>-66498125748364619</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"258.11\" cy=\"-44.03\" rx=\"44.03\" ry=\"44.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"258.11\" y=\"-38.23\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- 3716186020170190693 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3716186020170190693</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"285.11,-160.07 231.11,-160.07 231.11,-124.07 285.11,-124.07 285.11,-160.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"258.11\" y=\"-136.27\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- &#45;66498125748364619&#45;&gt;3716186020170190693 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>-66498125748364619-&gt;3716186020170190693</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.11,-88.55C258.11,-96.66 258.11,-104.95 258.11,-112.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.61,-112.36 258.11,-122.36 261.61,-112.36 254.61,-112.36\"/>\n",
       "</g>\n",
       "<!-- 5227787437159759806 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>5227787437159759806</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"258.11\" cy=\"-276.17\" rx=\"80.11\" ry=\"80.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"258.11\" y=\"-270.37\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">to_pyarrow_string</text>\n",
       "</g>\n",
       "<!-- 3716186020170190693&#45;&gt;5227787437159759806 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>3716186020170190693-&gt;5227787437159759806</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.11,-160.34C258.11,-167.1 258.11,-175.4 258.11,-184.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.61,-184.22 258.11,-194.22 261.61,-184.22 254.61,-184.22\"/>\n",
       "</g>\n",
       "<!-- 4172821046690527989 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4172821046690527989</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"436.11\" cy=\"-44.03\" rx=\"44.03\" ry=\"44.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"436.11\" y=\"-38.23\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- &#45;1176888008802505673 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>-1176888008802505673</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"463.11,-160.07 409.11,-160.07 409.11,-124.07 463.11,-124.07 463.11,-160.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"436.11\" y=\"-136.27\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 4172821046690527989&#45;&gt;&#45;1176888008802505673 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4172821046690527989-&gt;-1176888008802505673</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M436.11,-88.55C436.11,-96.66 436.11,-104.95 436.11,-112.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"432.61,-112.36 436.11,-122.36 439.61,-112.36 432.61,-112.36\"/>\n",
       "</g>\n",
       "<!-- 5068749226614284286 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>5068749226614284286</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"436.11\" cy=\"-276.17\" rx=\"80.11\" ry=\"80.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"436.11\" y=\"-270.37\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">to_pyarrow_string</text>\n",
       "</g>\n",
       "<!-- &#45;1176888008802505673&#45;&gt;5068749226614284286 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>-1176888008802505673-&gt;5068749226614284286</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M436.11,-160.34C436.11,-167.1 436.11,-175.4 436.11,-184.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"432.61,-184.22 436.11,-194.22 439.61,-184.22 432.61,-184.22\"/>\n",
       "</g>\n",
       "<!-- &#45;7189200816447331052 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>-7189200816447331052</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"614.11\" cy=\"-44.03\" rx=\"44.03\" ry=\"44.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"614.11\" y=\"-38.23\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- 5330752747299492752 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>5330752747299492752</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"641.11,-160.07 587.11,-160.07 587.11,-124.07 641.11,-124.07 641.11,-160.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"614.11\" y=\"-136.27\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- &#45;7189200816447331052&#45;&gt;5330752747299492752 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>-7189200816447331052-&gt;5330752747299492752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M614.11,-88.55C614.11,-96.66 614.11,-104.95 614.11,-112.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"610.61,-112.36 614.11,-122.36 617.61,-112.36 610.61,-112.36\"/>\n",
       "</g>\n",
       "<!-- 3386821119738866121 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>3386821119738866121</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"614.11\" cy=\"-276.17\" rx=\"80.11\" ry=\"80.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"614.11\" y=\"-270.37\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">to_pyarrow_string</text>\n",
       "</g>\n",
       "<!-- 5330752747299492752&#45;&gt;3386821119738866121 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>5330752747299492752-&gt;3386821119738866121</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M614.11,-160.34C614.11,-167.1 614.11,-175.4 614.11,-184.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"610.61,-184.22 614.11,-194.22 617.61,-184.22 610.61,-184.22\"/>\n",
       "</g>\n",
       "<!-- 5177257767402434271 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>5177257767402434271</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"792.11\" cy=\"-44.03\" rx=\"44.03\" ry=\"44.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.11\" y=\"-38.23\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- 7440036120417123049 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>7440036120417123049</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"819.11,-160.07 765.11,-160.07 765.11,-124.07 819.11,-124.07 819.11,-160.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.11\" y=\"-136.27\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">4</text>\n",
       "</g>\n",
       "<!-- 5177257767402434271&#45;&gt;7440036120417123049 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5177257767402434271-&gt;7440036120417123049</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M792.11,-88.55C792.11,-96.66 792.11,-104.95 792.11,-112.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"788.61,-112.36 792.11,-122.36 795.61,-112.36 788.61,-112.36\"/>\n",
       "</g>\n",
       "<!-- 3425514041675701871 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>3425514041675701871</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"792.11\" cy=\"-276.17\" rx=\"80.11\" ry=\"80.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.11\" y=\"-270.37\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">to_pyarrow_string</text>\n",
       "</g>\n",
       "<!-- 7440036120417123049&#45;&gt;3425514041675701871 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>7440036120417123049-&gt;3425514041675701871</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M792.11,-160.34C792.11,-167.1 792.11,-175.4 792.11,-184.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"788.61,-184.22 792.11,-194.22 795.61,-184.22 788.61,-184.22\"/>\n",
       "</g>\n",
       "<!-- &#45;9030167133868224737 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>-9030167133868224737</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"970.11\" cy=\"-44.03\" rx=\"44.03\" ry=\"44.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"970.11\" y=\"-38.23\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- 2546962091444426683 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>2546962091444426683</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"997.11,-160.07 943.11,-160.07 943.11,-124.07 997.11,-124.07 997.11,-160.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"970.11\" y=\"-136.27\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">5</text>\n",
       "</g>\n",
       "<!-- &#45;9030167133868224737&#45;&gt;2546962091444426683 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>-9030167133868224737-&gt;2546962091444426683</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M970.11,-88.55C970.11,-96.66 970.11,-104.95 970.11,-112.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"966.61,-112.36 970.11,-122.36 973.61,-112.36 966.61,-112.36\"/>\n",
       "</g>\n",
       "<!-- 7664833214114594479 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>7664833214114594479</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"970.11\" cy=\"-276.17\" rx=\"80.11\" ry=\"80.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"970.11\" y=\"-270.37\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">to_pyarrow_string</text>\n",
       "</g>\n",
       "<!-- 2546962091444426683&#45;&gt;7664833214114594479 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>2546962091444426683-&gt;7664833214114594479</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M970.11,-160.34C970.11,-167.1 970.11,-175.4 970.11,-184.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"966.61,-184.22 970.11,-194.22 973.61,-184.22 966.61,-184.22\"/>\n",
       "</g>\n",
       "<!-- &#45;810036887397515834 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>-810036887397515834</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"107.11,-428.28 53.11,-428.28 53.11,-392.28 107.11,-392.28 107.11,-428.28\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.11\" y=\"-404.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- &#45;1856934773411932769&#45;&gt;&#45;810036887397515834 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>-1856934773411932769-&gt;-810036887397515834</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.11,-356.69C80.11,-365.23 80.11,-373.46 80.11,-380.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"76.61,-380.58 80.11,-390.58 83.61,-380.58 76.61,-380.58\"/>\n",
       "</g>\n",
       "<!-- 5697603868704482591 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>5697603868704482591</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"285.11,-428.28 231.11,-428.28 231.11,-392.28 285.11,-392.28 285.11,-428.28\"/>\n",
       "<text text-anchor=\"middle\" x=\"258.11\" y=\"-404.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 5227787437159759806&#45;&gt;5697603868704482591 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>5227787437159759806-&gt;5697603868704482591</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.11,-356.69C258.11,-365.23 258.11,-373.46 258.11,-380.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.61,-380.58 258.11,-390.58 261.61,-380.58 254.61,-380.58\"/>\n",
       "</g>\n",
       "<!-- 804529839731786225 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>804529839731786225</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"463.11,-428.28 409.11,-428.28 409.11,-392.28 463.11,-392.28 463.11,-428.28\"/>\n",
       "<text text-anchor=\"middle\" x=\"436.11\" y=\"-404.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 5068749226614284286&#45;&gt;804529839731786225 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>5068749226614284286-&gt;804529839731786225</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M436.11,-356.69C436.11,-365.23 436.11,-373.46 436.11,-380.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"432.61,-380.58 436.11,-390.58 439.61,-380.58 432.61,-380.58\"/>\n",
       "</g>\n",
       "<!-- 2913813212849416522 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>2913813212849416522</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"641.11,-428.28 587.11,-428.28 587.11,-392.28 641.11,-392.28 641.11,-428.28\"/>\n",
       "<text text-anchor=\"middle\" x=\"614.11\" y=\"-404.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 3386821119738866121&#45;&gt;2913813212849416522 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>3386821119738866121-&gt;2913813212849416522</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M614.11,-356.69C614.11,-365.23 614.11,-373.46 614.11,-380.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"610.61,-380.58 614.11,-390.58 617.61,-380.58 610.61,-380.58\"/>\n",
       "</g>\n",
       "<!-- &#45;9025290104758136669 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>-9025290104758136669</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"819.11,-428.28 765.11,-428.28 765.11,-392.28 819.11,-392.28 819.11,-428.28\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.11\" y=\"-404.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">4</text>\n",
       "</g>\n",
       "<!-- 3425514041675701871&#45;&gt;&#45;9025290104758136669 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>3425514041675701871-&gt;-9025290104758136669</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M792.11,-356.69C792.11,-365.23 792.11,-373.46 792.11,-380.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"788.61,-380.58 792.11,-390.58 795.61,-380.58 788.61,-380.58\"/>\n",
       "</g>\n",
       "<!-- 4528379939978718581 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>4528379939978718581</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"997.11,-428.28 943.11,-428.28 943.11,-392.28 997.11,-392.28 997.11,-428.28\"/>\n",
       "<text text-anchor=\"middle\" x=\"970.11\" y=\"-404.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">5</text>\n",
       "</g>\n",
       "<!-- 7664833214114594479&#45;&gt;4528379939978718581 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>7664833214114594479-&gt;4528379939978718581</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M970.11,-356.69C970.11,-365.23 970.11,-373.46 970.11,-380.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"966.61,-380.58 970.11,-390.58 973.61,-380.58 966.61,-380.58\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.visualize(filename=\"../img/ch-dask-dataframe/nyc-flights-graph\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个例子中，Dask 根据文件数量来确定 Partition 的数量。{numref}`dask-task-graph` 提到，数据切分得过细或者过粗都不是最好的。对于所有可能的场景来说，单纯根据文件数量来确定切分为多少个 Partition 的方式并不一定是最优的：因为如果很多小文件，切分粒度会过细；或者如果是单个大文件，切分粒度过粗，可能导致 OOM。这两种极端情况下生成的 Task Graph 和并行粒度都不是最优的。所以，Dask DataFrame 的 `read_csv()` 提供了自定义每个 Partition 大小的参数 `blocksize`，单个 Partition 的大小不会超过 `blocksize`。如果用户没有显式设置 `blocksize`，Dask DataFrame 会根据探测到的计算资源情况来确定 `blocksize`，最大不超过 64MB。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据表模式推理\n",
    "\n",
    "在读取这些数据时，Dask DataFrame 对数据的类型和表模式进行了推测，并将字段名、字段对应的数据类型等表模式记录下来。Dask DataFrame 的 `read_csv()` 有 `sample` 的参数，表示只读取前面 `sample` 字节大小的数据，并根据这些数据来推测数据类型。这会带来下面的问题："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取 `ddf` 的最后 3 行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 17:43:50,091 - distributed.worker - WARNING - Compute Failed\n",
      "Key:       ('to_pyarrow_string-be40c5ef4347788c673803e766c9f5ac', 5)\n",
      "Function:  execute_task\n",
      "args:      ((subgraph_callable-9daf7b14-cf6d-4b93-b345-9197c71b1866, [(<function read_block_from_file at 0x119cfd3a0>, <OpenFile '/Users/luweizheng/Projects/godaai/distributed-python/ch-dask-dataframe/../data/nyc-flights/1996.csv'>, 0, 24979433, b'\\n'), None, True, True]))\n",
      "kwargs:    {}\n",
      "Exception: 'ValueError(\\'Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\\\\n\\\\n+----------------+---------+----------+\\\\n| Column         | Found   | Expected |\\\\n+----------------+---------+----------+\\\\n| CRSElapsedTime | float64 | int64    |\\\\n| TailNum        | object  | float64  |\\\\n+----------------+---------+----------+\\\\n\\\\nThe following columns also raised exceptions on conversion:\\\\n\\\\n- TailNum\\\\n  ValueError(\"could not convert string to float: \\\\\\'N14346\\\\\\'\")\\\\n\\\\nUsually this is due to dask\\\\\\'s dtype inference failing, and\\\\n*may* be fixed by specifying dtypes manually by adding:\\\\n\\\\ndtype={\\\\\\'CRSElapsedTime\\\\\\': \\\\\\'float64\\\\\\',\\\\n       \\\\\\'TailNum\\\\\\': \\\\\\'object\\\\\\'}\\\\n\\\\nto the call to `read_csv`/`read_table`.\\')'\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/4n/v40br47s46ggrjm9bdm64lwh0000gn/T/ipykernel_35647/3690877535.py\", line 3, in <module>\n",
      "    ddf.tail(3)\n",
      "  File \"/Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/dask/dataframe/core.py\", line 1591, in tail\n",
      "    result = result.compute()\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/dask/base.py\", line 342, in compute\n",
      "    (result,) = compute(self, traverse=False, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/dask/base.py\", line 628, in compute\n",
      "    results = schedule(dsk, keys, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/dask/dataframe/io/csv.py\", line 142, in __call__\n",
      "    df = pandas_read_text(\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/dask/dataframe/io/csv.py\", line 197, in pandas_read_text\n",
      "    coerce_dtypes(df, dtypes)\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/dask/dataframe/io/csv.py\", line 298, in coerce_dtypes\n",
      "    raise ValueError(msg)\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n",
      "\n",
      "+----------------+---------+----------+\n",
      "| Column         | Found   | Expected |\n",
      "+----------------+---------+----------+\n",
      "| CRSElapsedTime | float64 | int64    |\n",
      "| TailNum        | object  | float64  |\n",
      "+----------------+---------+----------+\n",
      "\n",
      "The following columns also raised exceptions on conversion:\n",
      "\n",
      "- TailNum\n",
      "  ValueError(\"could not convert string to float: 'N14346'\")\n",
      "\n",
      "Usually this is due to dask's dtype inference failing, and\n",
      "*may* be fixed by specifying dtypes manually by adding:\n",
      "\n",
      "dtype={'CRSElapsedTime': 'float64',\n",
      "       'TailNum': 'object'}\n",
      "\n",
      "to the call to `read_csv`/`read_table`.\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "try:\n",
    "    ddf.tail(3)\n",
    "except Exception:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`traceback` 将 `ddf.tail(3)` 抛出的异常捕获。为什么抛出了异常？因为 Dask DataFrame 并没有把所有数据都读取出来，而只是读取了 `sample` 大小的数据。恰好这些数据中，`CRSElapsedTime` 列是空的，Dask DataFrame 猜测该字段为 `float64` 类型，但后面的行有数据了，是 `int64` 类型。或者说，Dask DataFrame 猜测的字段数据类型和最后实际遇到的数据类型不一致。\n",
    "\n",
    "针对这种情况，应该在最开始读取数据的时候就明确数据类型，而不是依赖 Dask 的表模式推测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259511</th>\n",
       "      <td>1996-12-29</td>\n",
       "      <td>7</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1640</td>\n",
       "      <td>2236.0</td>\n",
       "      <td>1852</td>\n",
       "      <td>UA</td>\n",
       "      <td>1659</td>\n",
       "      <td>N424UA</td>\n",
       "      <td>268.0</td>\n",
       "      <td>...</td>\n",
       "      <td>225.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259512</th>\n",
       "      <td>1996-12-30</td>\n",
       "      <td>1</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>1640</td>\n",
       "      <td>1855.0</td>\n",
       "      <td>1852</td>\n",
       "      <td>UA</td>\n",
       "      <td>1659</td>\n",
       "      <td>N401UA</td>\n",
       "      <td>251.0</td>\n",
       "      <td>...</td>\n",
       "      <td>235.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259513</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>1640</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>1852</td>\n",
       "      <td>UA</td>\n",
       "      <td>1659</td>\n",
       "      <td>N414UA</td>\n",
       "      <td>243.0</td>\n",
       "      <td>...</td>\n",
       "      <td>216.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>LGA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "259511 1996-12-29          7   2008.0        1640   2236.0        1852   \n",
       "259512 1996-12-30          1   1644.0        1640   1855.0        1852   \n",
       "259513 1996-12-31          2   1659.0        1640   1902.0        1852   \n",
       "\n",
       "       UniqueCarrier  FlightNum TailNum  ActualElapsedTime  ...  AirTime  \\\n",
       "259511            UA       1659  N424UA              268.0  ...    225.0   \n",
       "259512            UA       1659  N401UA              251.0  ...    235.0   \n",
       "259513            UA       1659  N414UA              243.0  ...    216.0   \n",
       "\n",
       "        ArrDelay  DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  \\\n",
       "259511     224.0     208.0     LGA  DEN   1619.0     8.0     35.0      False   \n",
       "259512       3.0       4.0     LGA  DEN   1619.0     5.0     11.0      False   \n",
       "259513      10.0      19.0     LGA  DEN   1619.0     5.0     22.0      False   \n",
       "\n",
       "        Diverted  \n",
       "259511         0  \n",
       "259512         0  \n",
       "259513         0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dd.read_csv(file_path,\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': object,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "ddf.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依然使用刚才的例子，我们加上 `blocksize` 参数，分别设置为 `50_000` 和 `5_000_000`，比较一下不同 Partition 下计算速度，也可以观察 Dask 仪表盘上提供的各类信息。`blocksize` 为 `50_000` 时，Task Graph 切分粒度更细，计算图更大，复杂的计算图带来额外开销使得总体耗时更长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin\n",
      "EWR    669254\n",
      "JFK    255485\n",
      "LGA    591305\n",
      "Name: Origin, dtype: int64\n",
      "CPU times: user 1.77 s, sys: 701 ms, total: 2.47 s\n",
      "Wall time: 5.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.read_csv(file_path, \n",
    "                parse_dates={\"Date\": [0, 1, 2]},\n",
    "                dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n",
    "                blocksize=50_000)\n",
    "\n",
    "origin_cnt = ddf[~ddf.Cancelled].groupby(\"Origin\").Origin.count().compute()\n",
    "print(origin_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin\n",
      "EWR    669254\n",
      "JFK    255485\n",
      "LGA    591305\n",
      "Name: Origin, dtype: int64\n",
      "CPU times: user 48.6 ms, sys: 16.1 ms, total: 64.6 ms\n",
      "Wall time: 509 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.read_csv(file_path, \n",
    "                parse_dates={\"Date\": [0, 1, 2]},\n",
    "                dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n",
    "                blocksize=5_000_000)\n",
    "\n",
    "origin_cnt = ddf[~ddf.Cancelled].groupby(\"Origin\").Origin.count().compute()\n",
    "print(origin_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "在大数据工程领域，[Apache Parquet](https://parquet.apache.org/) 是一种被广泛应用的文件格式。相比 CSV 等纯文本类型文件，Parquet 有以下优势：\n",
    "\n",
    "* 列式存储\n",
    "* 内嵌表模式\n",
    "* 数据压缩\n",
    "\n",
    "具体而言，列式存储按照列进行存储，而不是 CSV 那样按行存储。数据分析时，我们可能只关心特定的列，而不是所有的列，因此在读取数据时，Parquet 允许很方便地过滤掉不需要的列，而不必读取整个行。因为减少了读取的数据量，Parquet 可以显著提高性能，也被广泛应用在 Apache Spark、Apache Hive 和 Apache Flink 等大数据生态。Parquet 自带了表模式，每个 Parquet 文件里嵌入了每个列的列名、数据类型等元数据，也就避免了 Dask DataFrame 进行表模式推测时推测不准确的问题。Parquet 中的数据是经过压缩的，相比 CSV，Parquet 更节省持久化存储的空间。\n",
    "\n",
    "比如，应该尽量读取所需要的列，而不是所有的列。\n",
    "\n",
    "```python\n",
    "dd.read_parquet(\n",
    "    \"s3://path/to/parquet/\",\n",
    "    columns=[\"a\", \"b\", \"c\"]\n",
    ")\n",
    "```\n",
    "\n",
    "此外，Parquet 提供了行分组（Row Group）的概念，如 {numref}`parquet-row-group` 所示，Parquet 文件中的数据是分组的，Row Group 定义了一个组内有多少行，这个例子中，一共 3 个 Row Group，每个 Row Group 有 2 行数据。每个 Row Group 存储了列的最大值和最小值等元数据，对这些列进行某些查询时，通过元数据的信息就可以确定是否读取这个 Row Group。比如，某时间列是一个时间序列，查询 “每天 9:00 至 12:00 的销量”，Row Group 中的元数据记录了时间列的最大值和最小值，通过元数据就可以判断是否有必要把这个 Row Group 读取出来，避免了读取不必要的开销。\n",
    "\n",
    "```{figure} ../img/ch-dask-dataframe/parquet-row-group.svg\n",
    "---\n",
    "width: 600px\n",
    "name: parquet-row-group\n",
    "---\n",
    "Parquet 列式存储与 Row Group\n",
    "```\n",
    "\n",
    "由于 Row Group 的引入，一个 Parquet 文件内可能有多个组，也就是说 Parquet 文件帮我们做了分组；不过很多 Parquet 文件的 Row Group 是 1。\n",
    "\n",
    "通常，数据集的文件被拆分为多个 Parquet，并且按照一定的方式来组织。比如，按照时间来切分：\n",
    "\n",
    "```\n",
    "/path/folder/\n",
    ".../year/month/day.parquet\n",
    "```\n",
    "\n",
    "Dask DataFrame 在读取 Parquet 数据集时，除了根据文件数量外，又多了一个可能的依据：Row Group。如果有 m 个文件，那 Partition 可能的选项有：\n",
    "\n",
    "* 每个 Parquet 文件对应 Dask 的一个 Partition，共有 m 个 Partition\n",
    "* 每个 Row Group 对应 Dask 的一个 Partition，假如一个 Parquet 文件有 n 个 Row Group，共有 m * n 个 Partition\n",
    "\n",
    "但无论哪种方式，应该尽量保证每个 Partition 所占用的内存空间不超过 Worker 的物理内存空间。Dask DataFrame 的\n",
    "`read_parquet()` 通过 `split_row_groups` 参数给用户更多选项。默认为 `split_row_groups=\"infer\"`：Dask DataFrame 根据 Parquet 文件中的元数据来推测，是每个文件对应一个 Partition，还是每个 Row Group 对应一个 Partition；如果设置为 `split_row_groups=True` 则强制每个 Row Group 对应一个 Partition；如果设置为 `False`，则每个文件对应一个 Partition。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
