# MPI 简介

Message Passing Interface（MPI）是个经典的并行计算工具，由于它的“年龄”比较老，新一代程序员很少听说过这个“老古董”，也经常忽视其重要性。但随着人工智能大模型浪潮的到来，MPI 或者基于 MPI 思想的各类通讯库再次回到人们的视线内，因为大模型必须使用并行计算框架进行跨机通信。比如，大模型训练框架 deepspeed 就使用了 mpi4py 进行多机通信。

## 历史

MPI 的发展可以追溯到20世纪80年代末和90年代初，彼时已经开始出现了超级计算机，主要用于科学和工程计算，包括气象模拟、核能研究、分子建模、流体动力学等领域。与现在的大数据集群一样，超级计算机主要是一组高性能计算机组成的集群。为了完成上述科学和工程计算问题，使得程序得以在多台计算机上并行运行。MPI 出现之前，多个研究小组和机构开始独立开发并推广自己的通信库，但这导致了互操作性和可移植性的问题。因此，社区迫切需要一种标准化的方法来编写并行应用程序。

1992年，图灵奖得主 Jack Dongarra 联合几位学者提出了并行计算第一个草案：MPI1。第一个标准版本 MPI 1.0 最终于 1994 年发布。之后，MPI-2、MPI-3 接连发布，来自学术界和工业界的多位专家共同参与修订，不断根据最新的并行计算需求修改 MPI 标准。

## 标准与实现

MPI 是一个标准，而不是一个具体的实现或者产品。像 Dask、Ray 这样的框架是一个具体的实现，而 MPI 不一样，MPI 是一个标准，不同厂商在这个标准下可以有自己的实现。“标准”的意思是说，MPI 定义了一些标准的函数或方法，所有的厂商都需要遵循；“实现”是说，不同软硬件厂商可以根据标准去实现底层通信。比如，如果实现一个发送数据的需求，MPI 标准中定义了 `MPI_Send` 方法，所有厂商应遵循这个标准。在具体实现上，现在常用的有 OpenMPI 、MPICH、Intel MPI 和 NVIDIA HPC-X 等。由于 MPI 是标准，因此，同样一份代码，可以被 OpenMPI 编译，也可以被 Intel MPI 编译。每个实现是由特定的厂商或开源社区开发的，因此使用起来也有一些差异。

## 高速网络

如果进行多机并行，机器之间需要有高速互联网络，比如 ROCE（RDMA over converged Ethernet）或 InfiniBand。如果你的集群已经部署了这些硬件，并安装了某个 MPI 实现，MPI 可以充分利用这些高速网络的高带宽、低延迟的特性。这些网络大部分拥有超过 100Gbps 的带宽，但其价格也非常昂贵，通常在面向高性能计算的场景上才会配置这些网络设备。

数据中心经常部署的万兆网络，带宽在 10Gbps 量级，也可以使用 MPI，并且会有一定加速效果。

MPI 也可以在单机上运行，即：利用单台节点上的多个计算核心。

## 安装

刚才提到，MPI 有不同的实现，即不同的 MPI 厂商一般会提供：

* 编译器 `mpicc`、`mpicxx` 和 `mpifort`，分别用来编译 C、C++、Fortran 语言编写的源代码，源代码中一部分是多机通讯，一部分是单机计算，这些编译器通常将多机通讯与单机计算的代码一起编译，并生成可执行文件。
* 在多台节点上将并行程序拉起的 `mpirun` 或 `mpiexec`。比如，在多少台节点上拉起多少进程等，都是通过 `mpirun` 来完成的。

如果使用 C/C++ 或 Fortran 这样的编译语言编写代码，一般的流程是：使用 `mpicc` 编译源代码，得到可执行文件，比如，将可执行文件命名为 `parallel.o`；使用 `mpirun` 在多台节点上将 `parallel.o` 并行程序拉起。mpi4py 将上述的编译环节封装。

如果你的集群环境已经安装了 MPI，可以先将 MPI 加载到环境变量里，然后使用 `pip` 安装：

```bash
pip install mpi4py
```

如果你的集群环境没有 MPI，而又对编译这些流程不熟悉，可以直接用 `conda` 安装。 使用 `conda` 安装的软件已经完成了编译过程。

```bash
conda install -c conda-forge mpich
conda install -c conda-forge mpi4py
conda install -c conda-forge ipyparallel
```

大部分 MPI 程序均需要在命令行中先编译再拉起。为解决这个问题，我们还安装了 ipyparallel，可以在 Jupyter Notebook 中完成并行程序的拉起。