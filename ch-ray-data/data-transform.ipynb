{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ray-data-transform)=\n",
    "# 数据转换\n",
    "\n",
    "数据处理的核心在于对数据进行一系列的转换（Transform），比如：\n",
    "\n",
    "* 如何对一条、一批次进行转换\n",
    "* 如何进行分组 `groupby`\n",
    "\n",
    "## 转换\n",
    "\n",
    "### map() 与 map_batches()\n",
    "\n",
    "Ray Data 提供了两类数据转换操作，如 {numref}`map-map-batches` 所示：\n",
    "\n",
    "* 每行数据，可以用 [`Dataset.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html) 和 [`Dataset.flat_map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.flat_map.html) 这两个 API，即对每一条数据一一进行转换。这与其他大数据框架（Spark 或者 Flink）类似。输入一条数据，输出一条数据。\n",
    "* 将多行数据打包为一个批次（Batch），对一个批次的数据进行转换：[`Dataset.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html)。输入一个 Batch，输出一个 Batch。\n",
    "\n",
    "```{figure} ../img/ch-ray-data/map-map-batches.svg\n",
    "---\n",
    "width: 800px\n",
    "name: map-map-batches\n",
    "---\n",
    "map() v.s. map_batches()\n",
    "```\n",
    "\n",
    "我们仍以纽约出租车数据为例，演示如何使用这两类转换操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-15 12:08:18,544\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-15 12:08:21,451\tINFO worker.py:1673 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 /Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/../data/nyc-taxi 已存在，无需操作。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()\n",
    "\n",
    "folder_path = os.path.join(os.getcwd(), \"../data/nyc-taxi\")\n",
    "download_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\"\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "parquet_file_path = os.path.join(folder_path, file_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    # 创建文件夹\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"文件夹 {folder_path} 不存在，已创建。\")\n",
    "    # 下载并保存 Parquet 文件\n",
    "    with urllib.request.urlopen(download_url) as response, open(parquet_file_path, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    print(\"数据已下载并保存为 Parquet 文件。\")\n",
    "else:\n",
    "    print(f\"文件夹 {folder_path} 已存在，无需操作。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据到 `Dataset` 类，先查看原有的数据格式，其中 `tpep_pickup_datetime` 和 `tpep_dropoff_datetime` 分别为乘客上车和下车时间，包含了日期和时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 12:08:23,601\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-15 12:08:26,697\tINFO dataset.py:2383 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-12-15 12:08:26,707\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 12:08:26,708\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 12:08:26,712\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=1]\n",
      "2023-12-15 12:08:26,713\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 12:08:26,714\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38102)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38102)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "                                                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'VendorID': 1,\n",
       "  'tpep_pickup_datetime': datetime.datetime(2023, 6, 1, 0, 8, 48),\n",
       "  'tpep_dropoff_datetime': datetime.datetime(2023, 6, 1, 0, 29, 41),\n",
       "  'passenger_count': 1,\n",
       "  'trip_distance': 3.4,\n",
       "  'RatecodeID': 1,\n",
       "  'store_and_fwd_flag': 'N',\n",
       "  'PULocationID': 140,\n",
       "  'DOLocationID': 238,\n",
       "  'payment_type': 1,\n",
       "  'fare_amount': 21.9,\n",
       "  'extra': 3.5,\n",
       "  'mta_tax': 0.5,\n",
       "  'tip_amount': 6.7,\n",
       "  'tolls_amount': 0.0,\n",
       "  'improvement_surcharge': 1.0,\n",
       "  'total_amount': 33.6,\n",
       "  'congestion_surcharge': 2.5,\n",
       "  'Airport_fee': 0.0}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ray.data.read_parquet(parquet_file_path)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Ray Data 的各类操作都是延迟（Lazy）执行的，即这些操作不是立即执行的，而是遇到数据查看或保存操作时，才会执行，比如：[`show()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.show.html)、[`take()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.take.html)、[`iter_rows()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.iter_rows.html)、[`write_parquet()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.write_parquet.html) 等操作会触发转换操作。\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `map()` 对这两个字段进行格式化，丢弃日期，只保留24小时制的时间。`map()` 的最重要的参数是一个自定义的函数 `fn`，这个函数对每一条数据进行转换，返回一条。输入数据有 Schema，每条数据是一个 Key-Value 的字典，Key 是 Schema 中的字段名，Value 是对应的数值。\n",
    "\n",
    "下面的例子，我们提取出了每次订单的时长、距离和价格，其他的字段先忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 12:08:28,216\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 12:08:28,219\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 12:08:28,221\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[Map(transform_row)] -> LimitOperator[limit=1]\n",
      "2023-12-15 12:08:28,222\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 12:08:28,224\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38108)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38108)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "                                                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_duration': 1253.0, 'trip_distance': 3.4, 'fare_amount': 21.9}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_row(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    result = {}\n",
    "    result[\"trip_duration\"] = (row[\"tpep_dropoff_datetime\"] - row[\"tpep_pickup_datetime\"]).total_seconds()\n",
    "    result[\"trip_distance\"] = row[\"trip_distance\"]\n",
    "    result[\"fare_amount\"] = row[\"fare_amount\"]\n",
    "    return result\n",
    "\n",
    "row_ds = dataset.map(transform_row)\n",
    "row_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 `map()` 有所区别的是，`map_batches()` 是对一个批次进行处理，它模拟的是单机处理时，对整个数据集的操作。其设计思想主要为了方便将之前编写好的、单机的程序，无缝地迁移到 Ray 上：用户先编写一个单机的程序，然后使用 Ray Data 迁移到集群上。`map_batches()` 每个批次的数据格式为 `Dict[str, np.ndarray]` 或 `pd.DataFrame` 或 `pyarrow.Table`，分别对应 NumPy 、pandas 和 Arrow。\n",
    "\n",
    "下面的例子与 `map()` 实现的功能类似，只不过通过 pandas 的形式，对每个 Batch 进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 12:08:35,838\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 12:08:35,839\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 12:08:35,840\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(transform_df)] -> LimitOperator[limit=10]\n",
      "2023-12-15 12:08:35,842\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 12:08:35,843\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38103)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38103)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "                                                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_duration': 1253, 'trip_distance': 3.4, 'fare_amount': 21.9},\n",
       " {'trip_duration': 614, 'trip_distance': 3.4, 'fare_amount': 15.6},\n",
       " {'trip_duration': 1123, 'trip_distance': 10.2, 'fare_amount': 40.8},\n",
       " {'trip_duration': 1406, 'trip_distance': 9.83, 'fare_amount': 39.4},\n",
       " {'trip_duration': 514, 'trip_distance': 1.17, 'fare_amount': 9.3},\n",
       " {'trip_duration': 796, 'trip_distance': 3.6, 'fare_amount': 18.4},\n",
       " {'trip_duration': 1136, 'trip_distance': 3.08, 'fare_amount': 19.8},\n",
       " {'trip_duration': 527, 'trip_distance': 1.1, 'fare_amount': 10.0},\n",
       " {'trip_duration': 237, 'trip_distance': 0.99, 'fare_amount': 6.5},\n",
       " {'trip_duration': 171, 'trip_distance': 0.73, 'fare_amount': 5.1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"trip_duration\"] = (input_df[\"tpep_dropoff_datetime\"] - input_df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    result_df[\"trip_distance\"] = input_df[\"trip_distance\"]\n",
    "    result_df[\"fare_amount\"] = input_df[\"fare_amount\"]\n",
    "    return result_df\n",
    "\n",
    "batch_ds = dataset.map_batches(transform_df, batch_format=\"pandas\")\n",
    "batch_ds.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实现 `map()` 或者 `map_batch()` 时，也可以使用 Python 的 lambda 表达式，即一个匿名的 Python 函数。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 12:08:37,202\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 12:08:37,203\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 12:08:37,205\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-12-15 12:08:37,207\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 12:08:37,208\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38106)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38106)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后的行数：730352\n"
     ]
    }
   ],
   "source": [
    "filterd_dataset = dataset.map_batches(lambda df: df[df[\"trip_distance\"] > 4], batch_format=\"pandas\")\n",
    "print(f\"过滤后的行数：{filterd_dataset.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 与 Actor\n",
    "\n",
    "可以看到，转换操作本质上是在执行 `fn` ，这个函数接收一个输入，进行转换，得到输出。默认情况下，Ray Data 使用 Task 并行执行转换操作。Ray Task 比较适合无状态的计算，即 `fn` 内不需要被不同数据反复依赖的数据。如果是有状态的计算，需要使用 Ray Actor。比如，加载一个机器学习模型，并用这个模型对所有数据进行预测。下面的例子模拟了机器学习模型预测的过程，模型本身是被反复使用的，所以是有状态的计算。这个例子仅仅作为演示，所使用的并非是训练好的模型，而是一个等价变换 `torch.nn.Identity()`，它将输入原封不动地转换为输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 12:08:44,228\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 12:08:44,228\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 12:08:44,229\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(transform_df)] -> LimitOperator[limit=100] -> ActorPoolMapOperator[MapBatches(TorchPredictor)] -> LimitOperator[limit=3]\n",
      "2023-12-15 12:08:44,230\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 12:08:44,230\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-12-15 12:08:44,276\tINFO actor_pool_map_operator.py:114 -- MapBatches(TorchPredictor): Waiting for 2 pool actors to start...\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38103)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=38103)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "2023-12-15 12:08:49,380\tWARNING actor_pool_map_operator.py:271 -- To ensure full parallelization across an actor pool of size 2, the Dataset should consist of at least 2 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'output': 3.4000000953674316},\n",
       " {'output': 3.4000000953674316},\n",
       " {'output': 10.199999809265137}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TorchPredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torch.nn.Identity()\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        pred = {}\n",
    "        inputs = torch.as_tensor(df['trip_distance'], dtype=torch.float32)\n",
    "        with torch.inference_mode():\n",
    "            pred[\"output\"] = self.model(inputs).detach().numpy()\n",
    "        return pred\n",
    "\n",
    "pred_ds = batch_ds.limit(100).map_batches(TorchPredictor, compute=ray.data.ActorPoolStrategy(size=2))\n",
    "pred_ds.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Actor 大概分为 3 步骤：\n",
    "\n",
    "1. 创建一个类，这个类包含一个 `__init__()` 方法和一个 `__call__()` 方法。`__init__()` 方法初始化一些可被反复使用的状态数据，`__call__()` 方法实现转换操作。可以参考刚才实现的 `TorchPredictor` 类。\n",
    "2. 创建一个 `ActorPoolStrategy`，指定一共多少个 Worker。\n",
    "3. 调用 `map_batch()` 方法，将 `ActorPoolStrategy` 传递给 `compute` 参数。\n",
    "\n",
    "## 分组\n",
    "\n",
    "数据处理中另外一个经常使用的原语是分组聚合，Ray Data 提供了： [groupby()](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html)。Ray Data 先调用 `groupby()`，对数据按照某些字段进行分组，再调用 [`map_groups()](https://docs.ray.io/en/latest/data/api/doc/ray.data.grouped_data.GroupedData.map_groups.html)` 对分组之后的数据进行聚合。\n",
    "\n",
    "`groupby(key)` 的参数 `key` 是需要进行分组的字段；`map_groups(fn)` 的参数 `fn`，对同一个组的数据进行操作。Ray Data 预置了一些聚合函数，比如常见的求和 `sum()`，最大值 `max()`，平均值 `mean()` 等。比如下面的例子使用 `mean()` 对 `value` 字段进行聚合。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 12:08:49,415\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=20]\n",
      "2023-12-15 12:08:49,415\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 12:08:49,417\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Sort Sample 0:   0%|          | 0/4 [00:00<?, ?it/s] object_store_memory:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return transform_pyarrow.concat(tables)\n",
      "                                                                                                               \n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group': 1, 'mean(value)': 1.5}\n",
      "{'group': 2, 'mean(value)': 3.5}\n"
     ]
    }
   ],
   "source": [
    "ds = ray.data.from_items([\n",
    "    {\"group\": 1, \"value\": 1},\n",
    "    {\"group\": 1, \"value\": 2},\n",
    "    {\"group\": 2, \"value\": 3},\n",
    "    {\"group\": 2, \"value\": 4}])\n",
    "mean_ds = ds.groupby(\"group\").mean(\"value\")\n",
    "mean_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
